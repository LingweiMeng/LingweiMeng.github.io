<!DOCTYPE html>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />

<html>

<head>
    <title>Lingwei Meng @ CUHK</title>
    <link rel="stylesheet" href="style.css">
    <link rel="icon" href="./img/icon.ico" type="image/x-icon">
    <link rel="shortcut icon" href="./img/icon.ico" type="image/x-icon">

</head>



<body>
    <div class="container">

        <div id="mypic">
            <!-- <img class="cir_pic" src="./img/IMG347.jpeg" alt="" id="picture"><br> -->
            <img class="cir_pic" src="./img/me.jpg" alt="" id="picture"><br>
        </div>

        <div id="header">
            <h1>Lingwei Meng</h1>
            <p style="margin-top: -5px;">
                Ph.D., The Chinese University of Hong Kong <br>
                <Strong>Email</Strong>: meng [at] link.cuhk.edu.hk<br>
            </p>
            <p style="margin-top: -13px;">
                <a href="https://scholar.google.com/citations?user=Vtirkf4AAAAJ&hl=en" target=_blank><strong>[Google Scholar]</strong></a> &ensp;
                <a href="https://www.linkedin.com/in/menglw/" target=_blank><strong>[LinkedIn]</strong></a> &ensp;
                <a href="https://github.com/LingweiMeng" target=_blank><strong>[GitHub]</strong></a> &ensp;
                <a href="https://x.com/LingweiMeng" target=_blank><strong>[ùïè]</strong></a> &ensp;
            </p>
        </div>

        <div class="clear"></div>

        <div id="about" , class="item">
            <h2>About</h2>
            <hr>
            <p>
                I received my Ph.D. degree in speech & language processing from <u>Human-Computer Communications Laboratory (HCCL), The Chinese University of Hong Kong (CUHK)</u>,
                supervised by <a href="https://www.se.cuhk.edu.hk/people/academic-staff/prof-meng-mei-ling-helen/"
                    target=_blank>Prof. Helen Meng</a>.
                <!-- margin 0.5 px -->
                Previously, I received my M.Phil. degree from the <u>Institute of Automation, Chinese Academy of Sciences (CASIA)</u>,
                where I was supervised by <a href="https://scholar.google.com/citations?user=nDcusNQAAAAJ&hl=en" target=_blank>Prof. Jie Tian</a>.
                And I received my Bachelor's degree from <u>Harbin Institute of Technology (HIT)</u>. I was a research intern at <u>Microsoft Research Asia</u>.
            </p>

            <p style="margin-top: -10px;"> My research interests focus on <strong>language modeling for speech synthesis</strong> and <strong>the
            integration of speech with large language models</strong>. I am also working on <strong>speech processing and recognition</strong>.</p>

            <!-- <p style="margin-top: -10px;">My recent research interests focus speech processing and recognition. I'm also interested in generative AI and integrating with large language models.</p> -->

            <!-- <p style="margin-top: 20px;"><strong>üü¢<i> &ensp; I am currently on the job market. Please drop me emails!</i></strong> </p> -->

        </div>


        <div id="news" , class="item">
            <h2>Highlights</h2>
            <hr>
            <p>
                
                <strong class="date">[Jul 2025]</strong>&ensp;
                    <strong>I defended my Ph.D. thesis</strong>
                <br>
                <strong class="date">[Jul 2025]</strong>&ensp;
                    <strong>Two papers, <a href="https://arxiv.org/abs/2502.11128" target=_blank>[FELLE]</a> & <a href="https://arxiv.org/abs/2504.10352" target=_blank>[PALLE]</a>, have been accepted to <i>ACM Multimedia 2025</i></strong>
                <br>
                <strong class="date">[May 2025]</strong>&ensp;
                    <strong>We present <a href="https://arxiv.org/abs/2407.08551" target=_blank>[MELLE]</a> (<i>ACL 2025 Main</i>), a pioneer effort in continuous-value token-based language modeling for TTS</strong>
                <br>
                <font class="date">[May 2025]</font>&ensp;
                    Three papers have been accepted to <i>ISCA INTERSPEECH 2025</i>
                <br>
                <font class="date">[Apr 2025]</font>&ensp;
                    Our <a href="https://arxiv.org/abs/2409.12388" target=_blank>[paper]</a> is the only winner of the <i>2025 IEEE Ganesh N. Ramaswamy Memorial Student Grant</i>
                <br>

                <font class="date">[Jan 2025]</font>&ensp;
                    We present <a href="https://openreview.net/forum?id=8pusxkLEQO" target=_blank>[ARLON]</a> (<i>ICLR 2025</i>), boosting diffusion transformers with autoregressive models for <i>long video generation</i>
                <br>
                <font class="date">[Dec 2024]</font>&ensp;
                     Two papers have been accepted to <i>IEEE ICASSP 2025</i>, including one first-authored <a href="https://arxiv.org/abs/2409.08596" target=_blank>[paper]</a>
                <br>
                <font class="date">[Dec 2024]</font>&ensp;
                    I participated in writing the excellent <a href="https://www.arxiv.org/abs/2412.18619" target=_blank>[survey]</a> on <i>Next Token Prediction Towards Multimodal Intelligence</i>
                <br>
                <!-- <font class="date">[Jul 2024]</font>&ensp;
                    We present <a href="https://arxiv.org/abs/2407.08551" target=_blank>[MELLE]</a>, a concise, continuous-value token-based language model for TTS, forgoing cumbersome NAR steps
                <br> -->
                <font class="date">[Jun 2024]</font>&ensp;
                    We propose <a href="https://arxiv.org/abs/2404.00656" target=_blank>[WavLLM]</a>, a robust and adaptive Speech LLM achieving SOTA performance on various speech-related tasks
                <br>

                <font class="date">[Jun 2024]</font>&ensp;
                    Three papers have been accepted to <i>ISCA INTERSPEECH 2024</i>, including one first-authored <a href="https://arxiv.org/abs/2407.09817" target=_blank>[paper]</a>
                <br>

                <font class="date">[Jan 2024]</font>&ensp;
                    Two papers have been accepted to <i>IEEE ICASSP 2024</i>
                <br>

                <!-- <font class="date">[Nov 2023]</font>&ensp;
                    Started an internship at <i><a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/" target=_blank>Microsoft Research Asia</a></i>.
                <br> -->
<!-- 
                <font class="date">[May 2023]</font>&ensp;
                    One first-authored <a href="https://arxiv.org/abs/2305.16263" target=_blank>[paper]</a> has been accepted to <i>ISCA INTERSPEECH 2023</i>
                <br>
                
                <font class="date">[Feb 2023]</font>&ensp;
                    One first-authored <a href="https://arxiv.org/abs/2302.09908" target=_blank>[paper]</a> has been accepted to <i>IEEE ICASSP 2023</i>
                <br> -->

                <!-- <font class="date">[Jun 2022]</font>&ensp;
                    Two papers have been accepted to <i>ISCA INTERSPEECH 2022</i>.
                <br>

                <font class="date">[Mar 2022]</font>&ensp;
                Our <a href="https://arxiv.org/abs/2210.16640" target=_blank>[paper]</a> was selected as an ‚≠êÔ∏è <a href="http://webofscience.help.clarivate.com/Content/esi-highly-cited-papers.html" target=_blank>ESI Highly Cited Paper</a> by <i>Web of Science</i>.
                <br>

                <font class="date">[Jan 2022]</font>&ensp; 
                    We won the 2nd place on the diarization track of the <i><a href="https://ieeexplore.ieee.org/document/9746465" target=_blank>IEEE ICASSP 2022 - M2Met Chanllenge</a></i>. This is our <a href="https://arxiv.org/abs/2202.01986" target=_blank>[paper]</a>.
                <br> -->

                <!-- <strong>[06/2021]</strong> &ensp; I'm going to pursue my Ph.D. degree in The Chinese University of Hong Kong (CUHK) since this Sep.
                <br> -->

                <!-- <strong>[03/2021]</strong> I am looking for an opportunity for the 2021 Fall Ph.D. program. I would greatly appreciate it if you drop me emails.</font>   -->

            </p>

        </div>

        <div id="education" , class="item">
            <h2>Education</h2>
            <hr>
            <p>
                2021 - 2025, Ph.D., The Chinese University of Hong Kong (CUHK)
                <br> 2018 - 2021, M.Phil., Pattern Recognition and Intelligent Systems, Institute of Automation, Chinese Academy of Sciences (CASIA)
                <br> 2014 - 2018, B.Eng., Electrical Engineering, Harbin Institute of Technology (HIT)

            </p>
        </div>

        <div id="experiences" , class="item">
            <h2>Experiences</h2>
            <hr>

            <ul class="mylist">
                <li>
                    <span>Nov. 2023 - Feb. 2025</span><strong>Research Intern, General Artificial Intelligence Group, <a href="https://www.microsoft.com/en-us/research/group/general-artificial-intelligence/" target=_blank>Microsoft Research Asia</a></strong>
                    <br> Studying language modeling for speech synthesis; integrating speech with large language models
                    <br> Working closely with <a href="https://scholar.google.com/citations?user=6mNya-wAAAAJ&hl=en" target=_blank>Dr. Shujie Liu</a>, <a href="https://long-zhou.github.io/" target=_blank>Dr. Long Zhou</a>, and <a href="https://thegenerality.com/" target=_blank>Dr. Furu Wei</a>
                </li>
            </ul>
        </div>

        <div id="publication" , class="item">
            <h2>Selected Publications</h2>
            <hr>
            <p>
                <a href="https://scholar.google.com/citations?user=Vtirkf4AAAAJ&hl=en" target=_blank> <strong>[Google Scholar] </strong></a>
            </p>

            <!-- <h4> - Speech related:</h4> -->

            <ul>
                <li>
                    <p><a href="https://arxiv.org/abs/2407.08551" target=_blank>MELLE: Autoregressive Speech Synthesis without Vector Quantization</a> &ensp; | &ensp; <a href="https://aka.ms/melle" target=_blank>[demo]</a>
                        <br><strong>Lingwei Meng</strong>, Long Zhou, Shujie Liu, Sanyuan Chen, Bing Han, Shujie Hu, Yanqing Liu, Jinyu Li, Sheng Zhao, Xixin Wu, <br>Helen Meng, Furu Wei
                        <br><i>The 63rd Annual Meeting of the Association for Computational Linguistics (ACL), 2025</i>
                    </p>
                </li>

                <li>
                    <p><a href="https://arxiv.org/abs/2409.08596" target=_blank>Large Language Model Can Transcribe Speech in Multi-Talker Scenarios with Versatile Instructions</a> &ensp; | &ensp; <a href="https://github.com/cuhealthybrains/MT-LLM" target=_blank>[code]</a>
                        <br><strong>Lingwei Meng</strong>, Shujie Hu, Jiawen Kang, Zhaoqing Li, Yuejiang Wang, Wenxuan Wu, Xixin Wu, Xunying Liu, Helen Meng
                        <br><i>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, 2025
                    </p>
                </li>

                <li>
                    <p><a href="https://arxiv.org/abs/2407.09817" target=_blank>Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System</a> &ensp; | &ensp; <a href="https://github.com/LingweiMeng/Whisper-Sidecar" target=_blank>[code]</a>
                        <br><strong>Lingwei Meng</strong>, Jiawen Kang, Yuejiao Wang, Zengrui Jin, Xixin Wu, Xunying Liu, Helen Meng
                        <br><i>ISCA INTERSPEECH</i>, 2024
                    </p>
                </li>

                <li>
                    <p><a href="https://arxiv.org/abs/2302.09908" target=_blank>A Sidecar Separator Can Convert a Single-Talker
                            Speech Recognition System to
                            a Multi-Talker One</a> &ensp; | &ensp; <a href="./pdf/ICASSP2023_Sidecar_video_slides.pdf"
                            target=_blank>[slides]</a>&ensp;<a href="https://youtu.be/NHKuRBQkTP4" target=_blank>[video]</a>
                        <br><strong>Lingwei Meng</strong>, Jiawen Kang, Mingyu Cui, Yuejiao Wang, Xixin Wu, Helen Meng
                        <br><i>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, 2023
                        <!-- <br><span class="highlight">(Oral Presentation)</span> -->
                    </p>
                </li>

                <li>
                    <p><a href="https://arxiv.org/abs/2305.16263" target=_blank>Unified Modeling of Multi-Talker 
                        Overlapped Speech Recognition and Diarization with a Sidecar Separator</a>
                        <br><strong>Lingwei Meng</strong>, Jiawen Kang, Mingyu Cui, Haibin Wu, Xixin Wu, Helen Meng
                        <br><i>ISCA INTERSPEECH</i>, 2023
                    </p>
                </li>

                <li>
                    <p><a href="https://arxiv.org/abs/2407.08551" target=_blank>FELLE: Autoregressive Speech Synthesis with Token-Wise Coarse-to-Fine Flow Matching</a>
                        <br>Hui Wang, Shujie Liu, <strong>Lingwei Meng</strong>, Jinyu Li, Yifan Yang, Shiwan Zhao, Haiyang Sun, Yanqing Liu, Haoqin Sun, Jiaming Zhou, <br>Yan Lu, Yong Qin
                        <br><i>ACM MM</i>, 2025
                    </p>
                </li>

                <li>
                    <p><a href="https://arxiv.org/abs/2409.12388" target=_blank>Disentangling Speakers in Multi-Talker Speech Recognition with Speaker-Aware CTC</a>
                        <br>Jiawen Kang, <strong>Lingwei Meng</strong>, Mingyu Cui, Yuejiao Wang, Xixin Wu, Xunying Liu, Helen Meng
                        <br><i>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, 2025
                        <br><strong>(The only winner of the 2025 lEEE Ganesh N. Ramaswamy Memorial Student Grant)</strong>
                    </p>
                </li>

                <li>
                    <p><a href="https://arxiv.org/abs/2404.00656" target=_blank>ARLON: Boosting Diffusion Transformers with
                            Autoregressive Models for Long Video Generation</a> &ensp; | &ensp; <a href="https://arlont2v.github.io/"
                            target=_blank>[demo]</a>
                        <br>Zongyi Li, Shujie Hu, Shujie Liu, Long Zhou, Jeongsoo Choi, <strong>Lingwei Meng</strong>, Xun Guo, Jinyu
                        Li, Hefei Ling, Furu Wei
                        <br><i>The Thirteenth International Conference on Learning Representations (ICLR)</i>, 2024
                    </p>
                </li>

                <li>
                    <p><a href="https://arxiv.org/abs/2404.00656" target=_blank>WavLLM: Towards Robust and Adaptive Speech Large Language Model</a> &ensp; | &ensp; <a href="https://aka.ms/wavllm" target=_blank>[code]</a>
                        <br>Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, <strong>Lingwei Meng</strong>, Hongkun Hao, Jing Pan, Xunying Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, Furu Wei
                        <br><i>EMNLP Findings</i>, 2024
                    </p>
                </li>

                <li>
                    <p><a href="https://arxiv.org/abs/2406.07855" target=_blank>VALL-E R: Robust and Efficient Zero-Shot
                            Text-to-Speech Synthesis via Monotonic Alignment</a> &ensp; | &ensp; <a href="https://aka.ms/valler" target=_blank>[demo]</a>
                        <br>Bing Han, Long Zhou, Shujie Liu, Sanyuan Chen, <strong>Lingwei Meng</strong>, Yanming Qian, Yanqing Liu,
                        Sheng Zhao, Jinyu Li, Furu Wei
                        <br><i>arXiv</i>, 2024
                    </p>
                </li>

                <li>
                    <p><a href="https://arxiv.org/abs/2407.10376" target=_blank>Large Language Model-based FMRI Encoding of Language Functions for Subjects with Neurocognitive Disorder</a>
                        <br>Yuejiao Wang, Xianmin Gong, <strong>Lingwei Meng</strong>, Xixin Wu, Helen Meng
                        Sheng Zhao, Jinyu Li, Furu Wei
                        <br><i>ISCA INTERSPEECH</i>, 2024
                    </p>
                </li>

                <li>
                    <p><a href="https://arxiv.org/abs/2401.04152" target=_blank>Cross-Speaker Encoding Network for Multi-Talker Speech Recognition</a> &ensp; | &ensp; <a href="https://github.com/kjw11/CSEnet-ASR" target=_blank>[code]</a>
                        <br>Jiawen Kang, <strong>Lingwei Meng</strong>, Mingyu Cui, Haohan Guo, Xixin Wu, Xunying Liu, Helen Meng
                        <br><i>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, 2024
                    </p>
                </li>

                <li>
                    <p><a href="https://arxiv.org/abs/2401.14664" target=_blank>UNIT-DSR: Dysarthric Speech Reconstruction System Using Speech Unit Normalization</a>
                        <br>Yuejiao Wang, Xixin Wu, Disong Wang, <strong>Lingwei Meng</strong>, Helen Meng
                        <br><i>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, 2024
                    </p>
                </li>

                <!-- <li>
                    <p><a href="https://arxiv.org/abs/2305.12804" target=_blank>
                            The defender's perspective on automatic speaker verification: An overview
                        </a>
                        <br>Haibin Wu, Jiawen Kang, <strong>Lingwei Meng</strong>, Helen Meng, Hung-yi Lee
                        <br><i>International Joint Conferences on Artificial Intelligence (IJCAI) Workshop</i>,
                        Macao SAR, China, 2023
                    </p>
                </li> -->

                <li>
                    <p><a href="https://www.isca-archive.org/interspeech_2022/wu22g_interspeech.html"
                            target=_blank>Spoofing-Aware Speaker Verification by Multi-Level Fusion</a>
                        <br>Haibin Wu, <strong>Lingwei Meng</strong>, Jiawen Kang,
                        Jinchao Li, Xu Li, Xixin Wu, Hung-yi Lee, Helen Meng
                        <br><i>ISCA INTERSPEECH</i>, 2022
                    </p>
                </li>

                <li>
                    <p><a href="https://www.isca-archive.org/interspeech_2022/wang22l_interspeech.html"
                            target=_blank>Exploring Linguistic Feature and Model Combination
                            for Speech Recognition Based Automatic AD Detection</a>
                        <br> Yi Wang, Tianzi Wang, Zi Ye, <strong>Lingwei Meng</strong>, Shoukang Hu, Xixin Wu, Xunying
                        Liu, Helen Meng
                        <br><i>ISCA INTERSPEECH</i>, 2022
                    </p>
                </li>

                <li>
                    <p><a href="https://arxiv.org/abs/2202.01986" target=_blank>The CUHK-Tencent Speaker Diarization
                            System
                            for the ICASSP 2022 Multi-Channel Multi-Party Meeting Transcription Challenge</a>
                        <br>Naijun Zheng, Na Li, Xixin Wu, <strong>Lingwei Meng</strong>, Jiawen Kang, Haibin Wu, Chao
                        Weng, Dan Su, Helen Meng
                        <br><i>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>, 2022
                    </p>
                </li>

                <!-- <li>
                    <p><a href="https://www.isca-archive.org/odyssey_2022/wu22_odyssey.html"
                            target=_blank>Tackling Spoofing-Aware Speaker Verification with Multi-Model Fusion</a>
                        <br>Haibin Wu, Jiawen Kang, <strong>Lingwei Meng</strong>, Yang Zhang, Xixin Wu, Zhiyong Wu,
                        Hung-yi Lee, Helen Meng
                        <br><i>ISCA The Speaker and Language Recognition Workshop (Odyssey)</i>, Beijing, China, 2022
                    </p>
                </li> -->
            </ul>


            <h4 style="margin-top: 40px"> - Medical Image Analysis related journal papers: </h4>
            <ul>
                <li>
                    <p>
                        <a href="https://arxiv.org/abs/2210.16640" target=_blank>2D and
                            3D CT Radiomic
                            Features Performance Comparison in Characterization of Gastric Cancer: a Multi-Center
                            Study</a>
                        <br><strong>Lingwei Meng</strong>, Di
                        Dong, Xin Chen, Mengjie Fang, Rongpin Wang, Jing Li, Zaiyi Liu, Jie Tian
                        <br><i>IEEE Journal of Biomedical and Health Informatics (IEEE JBHI)</i>, 2021 &emsp;(Impact Factor: 6.7)
                        <br><strong>(ESI Highly Cited Paper)</strong>
                    </p>
                </li>

                <li>
                    <p><a href="https://ieeexplore.ieee.org/document/9241068" target=_blank>A Deep
                            Learning Prognosis
                            Model Help Alert for COVID-19 Patients at High-Risk of Death: a
                            Multi-Center Study</a>
                        <br><strong>Lingwei Meng</strong>, Di
                        Dong, Liang Li, Meng Niu, Yan Bai, Meiyun Wang, Xiaoming Qiu, Yunfei Zha, Jie Tian
                        <br><i>IEEE Journal of Biomedical and Health Informatics (IEEE JBHI)</i>, 2020 &emsp;(Impact Factor: 6.7)
                    </p>
                </li>

                <li>
                    <p><a href="https://bmcmedimaging.biomedcentral.com/articles/10.1186/s12880-020-00470-7"
                            target=_blank>Noninvasive Model
                            for Predicting Future Ischemic Strokes in Patients with Silent Lacunar Infarction Using
                            Radiomics</a>
                        <br> <strong>Lingwei Meng</strong>#, Jiehua Su#, Di Dong#, Wenyan Zhuo, Jianming Wang, Libin Liu, Yi
                        Qin, Ye Tian, Jie Tian, Zhaohui Li
                        <br><i>BMC Medical Imaging</i>, 2020 &#8195(# denotes the co-first authorship.) &emsp;(Impact Factor: 2.9)
                    </p>
                </li>

                <!-- <li>
                    <p><a href="https://aapm.onlinelibrary.wiley.com/doi/10.1002/mp.15437" target=_blank>Deep
                            Learning-Based AI Model for Signet-Ring Cell Carcinoma Diagnosis and Chemotherapy Response
                            Prediction in Gastric
                            Cancer
                        </a>
                        <br> Cong Li, Yun Qin, Wei-Han Zhang, Hanyu Jiang, Bin Song, Mustafa R. Bashir, Heng Xu, Ting
                        Duan, Mengjie Fang,
                        Lianzhen Zhong, <strong>Lingwei Meng</strong>, Di Dong, Zhenhua Hu, Jie Tian, Jian-Kun Hu
                        <br>
                        <i>Medical Physics</i>, 2022
                    </p>
                </li>

                <li>
                    <p><a href="https://linkinghub.elsevier.com/retrieve/pii/S1936523321001054" target=_blank>CT-based radiomic model at node level for the prediction of
                            normal-sized lymph node metastasis in
                            cervical cancer
                        </a>
                        <br> Yujia Liu, Huijian Fan, Di Dong, Ping Liu,
                        Bingxi He, <strong>Lingwei Meng</strong>, Jiaming Chen, Chunlin Chen, Jinghe Lang, Jie
                        Tian
                        <br>
                        <i>Translational Oncology</i>, 2021

                    </p>
                </li>

                <li>
                    <p><a href="https://link.springer.com/article/10.1007/s11432-020-2849-3" target=_blank>CT
                            Radiomics Can Help Screen the Coronavirus Disease 2019 (COVID-19): a Preliminary Study
                        </a>
                        <br> Mengjie Fang, Bingxi He, Li
                        Li, Di Dong, Xin Yang, Cong Li, <strong>Lingwei Meng</strong>, Lianzhen Zhong,
                        Hailin Li, Hongjun Li, Jie Tian
                        <br><i>Science China-Information Sciences</i>, 2020
                    </p>
                </li> -->


            </ul>
        </div>

        <div id="challenges" , class="item">
            <h2>Challenges</h2>
            <hr>
            <p>
            <ul class="mylist">
                <li>
                    <span>Rank 2/14</span><a href="https://ieeexplore.ieee.org/document/9746465" target=_blank>IEEE ICASSP 2022 Multi-channel Multi-party Meeting Transcription Challenge (M2MeT)</a> - Track 1
                </li>
            </ul>
            </p>
        </div>

        <div id="activities" , class="item">
            <h2>Activities</h2>
            <hr>
            <p>
                Serving as a reviewer for top journals and conferences, including IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP), IEEE Journal of Selected Topics in Signal Processing (JSTSP), IEEE Journal of Biomedical and Health Informatics (JBHI), Pattern Recognition, International Conference on Learning Representations (ICLR), IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), ISCA INTERSPEECH, ACM Multimedia, among others.
<!-- 
                Serving as a reviewer of
                <ul class="mylist" id="misclist">
                    <li>
                        <i>IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)</i>
                    </li>
                    <li>
                        <i>IEEE Journal of Selected Topics in Signal Processing (JSTSP)</i>
                    </li>
                    <li>
                        <i>IEEE Journal of Biomedical and Health Informatics (JBHI)</i>
                    </li>
                    <li>
                        <i>Pattern Recognition</i>
                    </li>
                </ul>

                <ul style="margin-top: -10px;">
                    <li>
                        <i>International Conference on Learning Representations (ICLR)</i>
                    </li>
                    <li>
                        <i>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</i>
                    </li>
                    <li>
                        <i>ISCA INTERSPEECH</i>
                    </li>
                    <li>
                        <i>ACM Multimedia</i>
                    </li>
                </ul>
                Committee Member of 2023 International Docotoral Forum -->
            </p>
        </div>

        <div id="ta" , class="item">
            <h2>Teaching Assistance</h2>
            <hr>
            <p>
                SEEM 3440, Operation Research II<br>
                AIST 3510 / SEEM 3510, Human-Computer Interaction
            </p>
        </div>

        <div id="misc" , class="item">
            <h2>Miscellaneous</h2>
            <hr>
            <p>
                Some awards on control algorithm and circuit design competitions:
                <ul class="mylist" id="misclist">
                    <li>
                        <span>First Prize (main contributor)</span> Aug 2018, the National Marine Vehicle Design Competition, China
                    </li>
                    <li>
                        <span>Second Prize (team leader)</span> Jun 2017, ABU Asia-Pacific Robot Contest (RoboCon), China Division
                    </li>
                    <li>
                        <span>Second Prize (team leader)</span> Jul 2016, NXP Cup National University Students Intelligent Car Race, China
                    </li>
                    <li>
                        <span>Second Prize (team leader)</span> Sep 2015, the National Undergraduate Electronics Design Contest, China
                    </li>
                </ul>
            </p>
        </div>

        <div id="footer">
            <p>Last updated in July 2025 <br>
                &copy; 2025 Lingwei Meng</p>
        </div>


    </div>

</body>

</html>
